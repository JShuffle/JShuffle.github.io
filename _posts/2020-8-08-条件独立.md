---
layout: post
title: 聊一聊Conditional independence
subtitle: 
date: 2020-8-08
categories: 概率论
tags: 概率论
---

# 聊一聊Conditional independence

条件独立是研究贝叶斯统计的底层基础，无论是做sampling时的MCMC还是贝叶斯网中的条件独立判断都需要我们事先了解条件独立到底是什么？

本文从介绍多个事件的独立性判断出发，简单谈谈事件、随机事件、随机变量的条件独立是什么。

随机事件的条件独立重点关注与pairwise independent的区别。

下面是几个主要结论：

1.Pairwise independence doesn’t imply independence  

2.Conditional independence doesn’t imply independence  

## 3个事件的独立

下面是3个事件相互独立的判断依据：


$$
\begin{aligned}
P(A \cap B) &=P(A) P(B) \\
P(A \cap C) &=P(A) P(C) \\
P(B \cap C) &=P(B) P(C) \\
P(A \cap B \cap C) &=P(A) P(B) P(C)
\end{aligned}
$$


4条公式缺一不可：

1.如果只有前三个，我们只能说A,B,C是pairwise independent；pairwise independent的意思是只知道A不对C造成影响以及B不会对C造成影响，但无法确定**A与B两个事件一起发生是否会对C造成影响。**

下面是一个例子：

有2个硬币，记A事件为第一枚硬币朝上，B为第二枚硬币朝上，C为两枚硬币结果相同。A与B，A与C，B与C是两两独立的。（知道了第一枚硬币朝上并不能帮助我们更好地知道另一枚也会朝上），但是不是就是独立呢？不是的，因为：

P(ABC)=P(AB)=1/4

P(A)P(B)P(C)=1/4 * 1/2 = 1/8

显然第四条公式不满足。直观理解，在知道了A B事件发生情况后，C事件是否发生就知道了；但如果只知道A或者B，我们则无法知道C是否会发生。



## 多个事件的独立

把3个事件推广到更多的事件，假设有n个事件，记作A1,A2,....,An；必须要同时满足任意事件两两独立（任意事件的同时发生都是独立的），任意三三独立（任意三个事件的同时发生都是独立的），四四独立，....，n个事件同时发生都独立，即：


$$
P\left(A_{i} \cap A_{j}\right)=P\left(A_{i}\right) P\left(A_{j}\right)(\text { for } i \neq j) \\
P\left(A_{i} \cap A_{j} \cap A_{k}\right)=P\left(A_{i}\right) P\left(A_{j}\right) P\left(A_{k}\right) \text { (for } i, j, k \text { distinct }) \\
... \\
P\left(A_{1} \cap A_{2} \cap...\cap A_{n}\right)=P\left(A_{1}\right) P\left(A_{2}\right)...P\left(A_{n}\right)
$$


才能说这n个事件是独立的。



## 随机事件的条件独立

记号:


$$
(A \perp B)|C \quad \Longleftrightarrow \quad \operatorname{Pr}(A \cap B | C)=\operatorname{Pr}(A | C) \operatorname{Pr}(B | C)
$$


其表示事件A与事件B在给定事件C发生的前提下是独立的。

根据维基百科，条件独立的另一种表示方式是：


$$
(A \perp B)|C \quad \Longleftrightarrow \quad \operatorname{Pr}(A | B \cap C)=\operatorname{Pr}(A | C) \quad \text { or } \quad \operatorname{Pr}(B | C)=1
$$


这个公式初看不好理解，但不妨这么想：在给定C的条件下，A与B是独立的；因此在给定C的条件下，给定B并不会帮助我们增加对A的预测，因此`P(A|B,C)`=`P(A|C)`；

证明：


$$
\begin{aligned}
P(A | B \cap C) &=\frac{P(A \cap B \cap C)}{P(B \cap C)} \\
&=\frac{P(A \cap B | C) \cdot P(C)}{P(B \cap C)} \\
&=\frac{P(A | C) \cdot P(B | C) \cdot P(C)}{P(B | C) \cdot P(C)} \\
&=P(A | C)
\end{aligned}
$$



条件独立不能与独立划等号。比如：

1.两个事件可以（在给定E事件发生的情况下）条件独立，但不意味着这俩事件是独立的。

2.俩事件是独立的，并不意味着可以（在给定E事件发生的情况下）条件独立。

3.两个事件可以（在给定E事件发生的情况下）条件独立，但不意味着这俩事件在给定E不发生的情况下是条件独立的。

**Two events can be conditionally independent given E, but not independent. **

**Two events can be independent, but not conditionally independent given E.** 

**Two events can be conditionally independent given E,but not independent given Ec.**   



下面是几个例子：

**1.Two events can be conditionally independent given E, but not independent. **

考虑一个最极端的情况，假设有2枚硬币，第一枚朝上概率是1/2，第二枚是1。

令事件：

A=前10次抛硬币，结果都朝上

B=第11次抛硬币，结果朝上

C=硬币来自第一枚

我们不难得出，在`P(A,B|C）`=`P(A|C)xP(B|C)`，即A与B条件独立，因为都来自第一枚硬币，那么不同次数之间的抛硬币实验显然是相互独立的。

但如果没有条件C，P(B|A)显然不等于P(B)，这是因为，A的条件带来很大的信息量（你前十次都朝上，有理由认为第十一次朝上的概率也很大！），导致A与B不独立。

下面是一个小实例：

有2个硬币，第一枚正面朝上的概率是1/2，第2枚是1；现在，随机抽取一枚硬币，进行2重伯努利试验。

事件A=第一次伯努利试验结果朝上

事件B=第2次伯努利试验结果朝上

事件E=硬币来自第一枚

那么，显然：`P(A|E)` =1/2 `P(B|E) `=1/2 ，但A与B并不独立。

因为：

不妨首先计算P(A)


$$
\begin{aligned}
P(A) &=P(A | C) P(C)+P\left(A | C^{c}\right) P\left(C^{c}\right) \\
&=\frac{1}{2} \cdot \frac{1}{2}+1 \cdot \frac{1}{2} \\
&=\frac{3}{4}
\end{aligned}
$$


同理P(B)=3/4

下面，来看看P(A,B)=？ 即两次都朝上的概率？

利用全概率公式：


$$
\begin{aligned}
P(A \cap B) &=P(A \cap B | C) P(C)+P\left(A \cap B | C^{c}\right) P\left(C^{c}\right) \\
&=P(A | C) P(B | C) P(C)+P\left(A | C^{c}\right) P\left(B | C^{c}\right) P\left(C^{c}\right)
\\ 
&=\frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2}+1 \cdot 1 \cdot \frac{1}{2}\\
&=\frac{5}{8}

\end{aligned}
$$


其中，`P(A,B|comp(C)) = P(A|comp(C)) x P(B|comp(C))`是依据条件独立的性质推出来的。（在给定硬币来自第二枚的情况下，第一次和第二次朝上的概率是独立的）

显然，结果表明P(A,B) 不等于P(A)xP(B).

因此结论是：**A和B事件仅在给定C或者comp(C)的情况下条件独立，但本身并不独立。**

> https://www.youtube.com/watch?v=TAyA-rjmesQ
>
> https://www.probabilitycourse.com/chapter1/1_4_4_conditional_independence.php







**2.Two events can be independent, but not conditionally independent given E. **

这一命题的例子非常好想，假设有3个事件:A,B,E，如果A与B是独立的，但E与A和B有着某种关系，这个时候，

P(A,B)=P(A)xP(B)  但`P(A,B|E)`不等于`P(A|E)xP(B|E)`

例子1：

A和B同学参加考试，他们的取得的分数多少是相互独立的。如果E事件是A和B的分数>180。那么，假设现在已经知道A考了90分，根据给定的事件E的条件，我们就可以知道B也考了90分以上。

这个例子说明即使A与B相互独立，但如果E事件与A和B有关，可能会使得A与B在给定E的条件下并不独立。

例子2：

A事件代表警报发生 B事件代表着火 C事件代表有贼，假设B和C相互独立，且B和C都会造成A发生。

那么在给定A的条件下，B和C还会独立吗？不独立！因为如果我们知道了C不发生，那就只能是B发生了！

即：`P（B|A,comp(C)）=1`

comp(C)代表补集，即C不发生。



**3.Two events can be conditionally independent given E,but not independent given Ec（补集）.   **

假设A事件代表同学1学习努利 B事件代表同学1取得高分成绩 E事件代表同学1在一个班级1

（假设在班级1，由于大家都成绩太差，因此教授随机给分；而如果不在班级1，教授给分与考试答对的题目成正比）

那么，在班级1，`P(A,B|E)=P(A|E)xP(B|E)`，即：如果同学1处在班级1，那么他学习努利和他取得高分是独立的。

而如果在其他班级，他学习努利和他取得高分是相关的。



## 随机变量的条件独立

随机变量X与Y在给定随机变量Z的条件下是独立的，可以表示为：


$$
P(X \leq x, Y \leq y | Z=z)=P(X \leq x | Z=z) P(Y \leq y | Z=z)
$$


即，无论随机变量Z取何值，都应该支持X与Y的条件独立性。

对于离散随机变量，可以写作：


$$
P(X=x, Y=y | Z=z)=P(X=x | Z=z) P(Y=y | Z=z)
$$


注意：**we are conditioning on Z = z everywhere, and require the equality to hold for all z
in the support of Z  **



## 参考

Introduction to Probability, Joseph Blitzstein

https://www.youtube.com/watch?v=TAyA-rjmesQ



